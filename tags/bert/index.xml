<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>BERT on 유기농은 너무 비싸서 그런데 농약 친 건 어딨나요?</title>
		<link>https://pko89403.github.io/tags/bert/</link>
		<description>Recent content in BERT on pko89403.github.io</description>
		<generator>Hugo -- gohugo.io</generator>
		<language>en-us</language>
		<copyright>© Copyright notice | Vitae theme for Hugo</copyright>
		<lastBuildDate>Sun, 09 May 2021 19:05:20 +0900</lastBuildDate>
		
		<atom:link href="https://pko89403.github.io/tags/bert/index.xml" rel="self" type="application/rss+xml" />
		
		<item>
			<title>BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
			<link>https://pko89403.github.io/post/bert/</link>
			<pubDate>Sun, 09 May 2021 19:05:20 +0900</pubDate>
			
			<guid>https://pko89403.github.io/post/bert/</guid>
			<description>
			
				BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding 리뷰 Abstract BERT라고 부르는 언어 표현 모델을 소개한다. BERT는 Bidirectional …
			
			</description>
		</item>
		
	</channel>
</rss>
