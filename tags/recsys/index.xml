<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Recsys on 유기농은 너무 비싸서 그런데 농약 친 건 어딨나요?</title>
    <link>https://pko89403.github.io/tags/recsys/</link>
    <description>Recent content in Recsys on 유기농은 너무 비싸서 그런데 농약 친 건 어딨나요?</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>© Copyright notice | [Vitae](https://github.com/dataCobra/hugo-vitae) theme for [Hugo](https://gohugo.io)</copyright>
    <lastBuildDate>Sun, 17 Apr 2022 19:03:56 +0900</lastBuildDate><atom:link href="https://pko89403.github.io/tags/recsys/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>KGAT: Knowledge Graph Attention Network for Recommendation 리뷰</title>
      <link>https://pko89403.github.io/post/kgat/</link>
      <pubDate>Sun, 17 Apr 2022 19:03:56 +0900</pubDate>
      
      <guid>https://pko89403.github.io/post/kgat/</guid>
      <description>ABSTRACT 더 정확하고, 다양하고, 설명가능한 추천을 제공하기 위해 유저-아이템 간 상호작용 모델링을 넘어 부가적인 정보도 함께 고려 해야 한다. FM 같은 기존 메소드는 지도 학습으로 각각의 상호작용을 부가 정보와 함께 인코딩된 독립된 객체로 가정했다. 하지만 기존 메소드는 인스턴스 혹은 아이템 간의 관계를 간과하기 때문에, CF 시그널을 녹여내기에 부족하다.
본 논문에서는 지식 그래프(KG)의 가능성을 연구 하는데, 지식 그래프로 독립적인 상호작용에 대한 가정을 아이템 어트리뷰트와 함께 연결시켜 세분화 할 수 있다고 한다. 하이브리드 구조인 KG와 유저-아이템 그래프에 연결된 어트리뷰트 들로 두 아이템을 연결하는 고차 관계성으로 성공적인 추천을 할수 있다고 주장한다.</description>
    </item>
    
    <item>
      <title>PUP : Price-aware Recommendation Convolution Network 리뷰</title>
      <link>https://pko89403.github.io/post/pup/</link>
      <pubDate>Sat, 19 Mar 2022 12:14:52 +0900</pubDate>
      
      <guid>https://pko89403.github.io/post/pup/</guid>
      <description>ABSTRACT 최근의 추천은 유저 행동을 마이닝 하는 연구가 많았다.
→ 유저 들이나 아이템 들을 설명하는 텍스트 정보, 데모그라픽, 이미지 등의 일반적인 정보를 다루는 CF.
마케팅에서 중요하게 여기는 가격은 유저의 최종적인 구매를 결정하지만, 상대적으로 관심을 받지 못했다.
→ 따라서 본 연구에서는 가격에 집중해서 유저의 구매 의도를 예측하는 추천 메소드 개발을 목표로 한다.
연구가 가지는 두가지 어려움은 다음과 같다.
 가격에 대한 유저의 선호도와 민감도가 알려지지 않았다. 과거 유저가 구매 아이템에 암시적으로 반영되어있다.</description>
    </item>
    
    <item>
      <title>GraphRec : Graph Neural Networks for Social Recommendation 리뷰</title>
      <link>https://pko89403.github.io/post/graphrec/</link>
      <pubDate>Wed, 09 Mar 2022 20:45:24 +0900</pubDate>
      
      <guid>https://pko89403.github.io/post/graphrec/</guid>
      <description>ABSTRACT 소셜 추천 시스템의 데이터는 아래와 같은 표현 된다.
 유저 - 유저 소셜 그래프 유저 - 아이템 그래프  그리고 핵심은 유저와 아이템의 Latent Factor로 학습하는 것이다.
그러나, GNNs으로 소셜 추천 시스템을 구축하는 것은 세가지 과제 있다.
 유저-아이템 그래프가 상호작용과 그에 연관된 오피니언을 포함한다 소셜 관계들이 서로 다른 강도를 가진다 유저가 유저-유저 소셜 그래프와 유저-아이템 그래프 모두에 포함된다  위의 세가지 문제를 해결하기 위해 논문이 기여하는 바는 아래와 같다.</description>
    </item>
    
    <item>
      <title>GMCF : Neural Graph Matching based Collaborative Filtering 리뷰</title>
      <link>https://pko89403.github.io/post/gmcf/</link>
      <pubDate>Wed, 09 Mar 2022 20:23:09 +0900</pubDate>
      
      <guid>https://pko89403.github.io/post/gmcf/</guid>
      <description>ABSTRACT 유저와 아이템의 어트리뷰트 들은 필수적인 사이드 정보이다. 본 논문에서는 기존 모델들이 구분하지 못하는 두 가지 어트리뷰트 상호작용을 발견했다.
 inner attribute interactions : 유저 어트리뷰트 사이의 상호작용과 아이템 어트리뷰트 간 상호작용 cross attribute interactions : 유저 어트리뷰트와 아이템 어트리뷰트 간 상호작용  따라서 그래프 매칭 구조를 사용해 어트리뷰트 상호작용을 모델링하고 집계해서 두 상호작용 타입을 효과적으로 포착하는 GMCF(neural Graph Matching based Collaborative Filtering) 모델을 추천에서 사용한다.
GMCF 모델은 두가지 과정을 수행하는데</description>
    </item>
    
    <item>
      <title>밴딧 추천 오픈소스 OBP의 Bandit Policy 에 대한 설명 : Policy</title>
      <link>https://pko89403.github.io/post/ope_policy/</link>
      <pubDate>Sat, 13 Nov 2021 16:05:55 +0900</pubDate>
      
      <guid>https://pko89403.github.io/post/ope_policy/</guid>
      <description>밴딧 추천 오픈소스 OBP(Open Bandit Pipeline)의 Bandit Policy 최근에 밴딧 기반 추천에 관심을 가지고 스터디에 참여하는 등의 활동을 진행하고 있다.
밴딧 기반 추천 시스템 도메인을 위해 일본의 패션 이커머스 플랫폼 ZOZOTOWN 에서 Open Bandit Dataset 라고 하는 수집된 데이터셋을 공개를 했다. 이 데이터셋은 동일한 플랫폼에서 각기 다른 정책을 동작 시키면서 수집된 많은 밴딧 피드백 로그 셋을 가지는 차별점이 있고 결론적으로 서로 다른 OPE 예측기들의 실험적 비교를 최초로 할 수 있게 해주었다.</description>
    </item>
    
    <item>
      <title>밴딧 추천 오픈소스 OBP의 Off-Policy-Evaluation 에 대한 설명 : Off-Policy Evaluation</title>
      <link>https://pko89403.github.io/post/ope_offpolicy/</link>
      <pubDate>Sat, 13 Nov 2021 16:05:47 +0900</pubDate>
      
      <guid>https://pko89403.github.io/post/ope_offpolicy/</guid>
      <description>&amp;ldquo;밴딧 추천 오픈소스 OBP의 Off-Policy-Evaluation 에 대한 설명 : Off-Policy Evaluation&amp;rdquo; Bandit 추천의 기반인 Policy에 대해 알아봤고 오프라인으로 로깅된 밴딧 피드백을 이용해 Policy의 값을 추정하는 추정기들과 그 추정기를 검증하는 Offline-Policy-Evaluation에 대해 실습을 진행한다. 아래의 공식 예제 코드를 약간 변형해서 진행하게 된다.
실습 진행 예제 코드
전체 코드의 실습은 OBP에서 지원하는 아래의 항목들을 사용했다.
 Dataset : SyntheticDataset (제공하는 랜덤 데이터셋 ) Policy : LinUCB Evaluation : RM, IPW  SyntheticBanditDataset을 온라인 밴딧 알고리즘의 OPE를 검증에 사용한다.</description>
    </item>
    
    <item>
      <title>A Contextual-Bandit Algorithm for Mobile Context-Aware Recommender System 논문 리뷰</title>
      <link>https://pko89403.github.io/post/mcrs/</link>
      <pubDate>Sun, 11 Jul 2021 22:04:33 +0900</pubDate>
      
      <guid>https://pko89403.github.io/post/mcrs/</guid>
      <description>Abastract 모바일 맥락 기반 추천 시스템의 대부분의 접근 방법은 맥락 정보를 활용해서 유저와 관련성 있는 아이템을 추천하는데 초점이 맞춰져 있다. 고려할만한 맥락 정보로 아래 세가지가 있다.
 시간 위치 소셜 정보  이전의 연구들은 모두 유저의 컨텐츠 변화 문제를 고려하지 않는데 이 논문은 컨텐츠에 대해 변화하는
 유저의 흥미를 다이나믹하게 처리하는 알고리즘을 제시 유저 상황에 따라 Exploration과 Exploitation을 결정한 뒤 적응형으로 EE 트레이드 오프  직접 설계한 오프라인 시뮬레이션 프레임워크로 실제 온라인 로그 데이터에서 검증을 수행한다.</description>
    </item>
    
    <item>
      <title>Collaborative Filtering Bandits 논문 리뷰</title>
      <link>https://pko89403.github.io/post/cofiba/</link>
      <pubDate>Sun, 27 Jun 2021 23:28:24 +0900</pubDate>
      
      <guid>https://pko89403.github.io/post/cofiba/</guid>
      <description>Collaborative Filtering Bandits 리뷰 ABSTRACT 기존 추천 방식
 Collaborative filtering Content-Based Filtering  주어진 학습 데이터로 부터 모델을 학습하는 메소드( 정적인 모델 ).
뉴스 추천 이나 광고 도메인에는 적용이 힘듬 → 아이템과 유저 셋의 계속된 변경
Contextual MAB 기반 적응형 클러스터링
유저 클러스터링 + 아이템 클러스터링
데이터 내의 선호 패턴을 이용한다. ( collaborative filtering )
SOTA
Regret 분석
1. INTRODUCTION Collarborative Filtering
아이템에 대한 상호작용한 정보를 사용한다.
유저 피처와 아이템 피처의 내적 메소드와 같은 기술로 재현하고,</description>
    </item>
    
    <item>
      <title>단단한 강화 학습 2장 다중 선택 2.6 ~ 정리</title>
      <link>https://pko89403.github.io/post/rl2.5.2/</link>
      <pubDate>Sun, 20 Jun 2021 22:11:13 +0900</pubDate>
      
      <guid>https://pko89403.github.io/post/rl2.5.2/</guid>
      <description>단단한 강화 학습 2장 다중 선택 2.6 ~ 정리 2.6 Optimal Initial Value Exploration을 촉진하는 기법을 긍정적 초깃값(Optimal initial value)라고 부른다.
이전에 언급된 방법들은 행동 가치의 초기 추정값 $Q_1(a)$의 영향을 받았다. 초깃값에 편중 되어(Biased) 있었다. 편향 되었다는게 나쁜 것만은 아니다. 편향된 정보를 사전 지식으로 초기 추정값을 정할 수 있는 장점이 있다.
액션 들의 가치에 대한 초기 추정값이 긍정적이면, 학습자는 액션에 대한 보상값에 실망해서 다른 행동을 선택하게 된다. 결과적으로 가치 추정값이 수렴하기 전까지 Exploration이 된다.</description>
    </item>
    
    <item>
      <title>단단한 강화 학습 2장 다중 선택 ~ 2.5 정리</title>
      <link>https://pko89403.github.io/post/rl2.5/</link>
      <pubDate>Sun, 13 Jun 2021 22:26:04 +0900</pubDate>
      
      <guid>https://pko89403.github.io/post/rl2.5/</guid>
      <description>Bandit Problem은 오직 하나의 상태만 다루는 강화학습 문제이다.
강화 학습은 지도 학습과는 다른 종류의 학습 피드백을 전달한다. 학습자의 행동과 무관하게 정해져 있는 정답이 있는 지도 학습과는 다르게, 강화 학습은 행동에 대한 결과를 피드백으로 전달한다. 그렇기 때문에 좋은 행동을 찾기 위한 직접적인 탐색이 필요하다.
단순화 된 구조에서 하나의 상황에 대해서만 행동을 학습하게 되는 구조를
Non-Associative 구조라고 한다.  하나 이상의 situation을 다루지 않음. Evaluative Feedback에 관한 작업이 이미 수행 된다.  이러한 구조를 가지는 특별한 구조로서 다중 선택 문제(Multi-Armed Bandits)가 있다.</description>
    </item>
    
    <item>
      <title>Artwork Personalization at Netflix</title>
      <link>https://pko89403.github.io/post/netflixartwork2/</link>
      <pubDate>Sun, 06 Jun 2021 22:41:41 +0900</pubDate>
      
      <guid>https://pko89403.github.io/post/netflixartwork2/</guid>
      <description>Artwork Personalization at Netflix 넷플릭스의 아트워크 추천에 대해서 알아보자. 블로그, PPT, 유튜브 자료가 있다.
Artwork Personalization at Netflix 블로그
qcon_artwork_personalization_talk PPT
Artwork Personalization at Netflix | Netflix 유투브 
Introduction 많은 시간 동안, 넷플릭스 개인화 추천 시스템의 목표는
 각 구성원에 적합한 타이틀을 적합한 시간에 제공  였다. 수 천 타이틀의 카탈로그와 일 억개가 넘는 계정의 회원으로, 회원 각각에게 적합한 타이틀을 추천하는 것은 중요하다. 그러나 추천 업무는 그걸로 끝이 아니다. 다음과 같은 질문에 답해야한다.</description>
    </item>
    
  </channel>
</rss>
