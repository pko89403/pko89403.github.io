<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>Bandits on 유기농은 너무 비싸서 그런데 농약 친 건 어딨나요?</title>
		<link>https://pko89403.github.io/tags/bandits/</link>
		<description>Recent content in Bandits on pko89403.github.io</description>
		<generator>Hugo -- gohugo.io</generator>
		<language>en-us</language>
		<copyright>© Copyright notice | Vitae theme for Hugo</copyright>
		<lastBuildDate>Sun, 20 Jun 2021 22:11:13 +0900</lastBuildDate>
		
		<atom:link href="https://pko89403.github.io/tags/bandits/index.xml" rel="self" type="application/rss+xml" />
		
		<item>
			<title>단단한 강화 학습 2장 다중 선택 2.6 ~ 정리</title>
			<link>https://pko89403.github.io/post/rl2.5.2/</link>
			<pubDate>Sun, 20 Jun 2021 22:11:13 +0900</pubDate>
			
			<guid>https://pko89403.github.io/post/rl2.5.2/</guid>
			<description>
			
				단단한 강화 학습 2장 다중 선택 2.6 ~ 정리 2.6 Optimal Initial Value Exploration을 촉진하는 기법을 긍정적 초깃값(Optimal initial value)라고 부른다. …
			
			</description>
		</item>
		<item>
			<title>단단한 강화 학습 2장 다중 선택 ~ 2.5 정리</title>
			<link>https://pko89403.github.io/post/rl2.5/</link>
			<pubDate>Sun, 13 Jun 2021 22:26:04 +0900</pubDate>
			
			<guid>https://pko89403.github.io/post/rl2.5/</guid>
			<description>
			
				Bandit Problem은 오직 하나의 상태만 다루는 강화학습 문제이다.
강화 학습은 지도 학습과는 다른 종류의 학습 피드백을 전달한다. 학습자의 행동과 무관하게 정해져 있는 정답이 있는 지도 학습과는 다르게, 강화 학습은 행동에 대한 결과를 피드백 …
			
			</description>
		</item>
		
	</channel>
</rss>
