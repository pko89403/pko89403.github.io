<!DOCTYPE html>
<html lang="en"><head>
	
	<meta name="generator" content="Hugo 0.82.1" />
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	
	<meta property="og:title" content="밴딧 추천 오픈소스 OBP의 Off-Policy-Evaluation 에 대한 설명 : Off-Policy Evaluation">
	
	
	<meta name="keywords" content="Recsys,Bandit,Yahoo,Open Bandit Pipeline,Off Policy Evaluation"><meta name="description" content="“밴딧 추천 오픈소스 OBP의 Off-Policy-Evaluation 에 대한 설명 : Off-Policy Evaluation” Bandit 추천의 기반인 Policy에 대해 알아봤고 오프라인으로 로깅된 밴딧 피드백을 이용해 Policy의 값을 추정하 …"><meta property="og:title" content="밴딧 추천 오픈소스 OBP의 Off-Policy-Evaluation 에 대한 설명 : Off-Policy Evaluation" />
<meta property="og:description" content="&ldquo;밴딧 추천 오픈소스 OBP의 Off-Policy-Evaluation 에 대한 설명 : Off-Policy Evaluation&rdquo; Bandit 추천의 기반인 Policy에 대해 알아봤고 오프라인으로 로깅된 밴딧 피드백을 이용해 Policy의 값을 추정하는 추정기들과 그 추정기를 검증하는 Offline-Policy-Evaluation에 대해 실습을 진행한다. 아래의 공식 예제 코드를 약간 변형해서 진행하게 된다.
실습 진행 예제 코드
전체 코드의 실습은 OBP에서 지원하는 아래의 항목들을 사용했다.
 Dataset : SyntheticDataset (제공하는 랜덤 데이터셋 ) Policy : LinUCB Evaluation : RM, IPW  SyntheticBanditDataset을 온라인 밴딧 알고리즘의 OPE를 검증에 사용한다." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://pko89403.github.io/post/ope_offpolicy/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2021-11-13T16:05:47&#43;09:00" />
<meta property="article:modified_time" content="2021-11-13T16:05:47&#43;09:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="밴딧 추천 오픈소스 OBP의 Off-Policy-Evaluation 에 대한 설명 : Off-Policy Evaluation"/>
<meta name="twitter:description" content="&ldquo;밴딧 추천 오픈소스 OBP의 Off-Policy-Evaluation 에 대한 설명 : Off-Policy Evaluation&rdquo; Bandit 추천의 기반인 Policy에 대해 알아봤고 오프라인으로 로깅된 밴딧 피드백을 이용해 Policy의 값을 추정하는 추정기들과 그 추정기를 검증하는 Offline-Policy-Evaluation에 대해 실습을 진행한다. 아래의 공식 예제 코드를 약간 변형해서 진행하게 된다.
실습 진행 예제 코드
전체 코드의 실습은 OBP에서 지원하는 아래의 항목들을 사용했다.
 Dataset : SyntheticDataset (제공하는 랜덤 데이터셋 ) Policy : LinUCB Evaluation : RM, IPW  SyntheticBanditDataset을 온라인 밴딧 알고리즘의 OPE를 검증에 사용한다."/>
<link rel="icon" type="image/png" href="/favicon-32x32.png" sizes="32x32">
	<link rel="icon" type="image/png" href="/favicon-16x16.png" sizes="16x16">
	<link rel="stylesheet" type="text/css" media="screen" href="/css/normalize.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="/css/main.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="/css/all.css" />
	<link rel="stylesheet" href="/css/katex.min.css" crossorigin="anonymous">
	<script defer src="/js/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
	<script defer src="/js/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
	<script>
		document.addEventListener("DOMContentLoaded", function() {
			renderMathInElement(document.body, {
				delimiters: [
					{left: "$$", right: "$$", display: true},
					{left: "$", right: "$", display: false}
				]
		});
		});
	</script><title>밴딧 추천 오픈소스 OBP의 Off-Policy-Evaluation 에 대한 설명 : Off-Policy Evaluation | 유기농은 너무 비싸서 그런데 농약 친 건 어딨나요?</title>


</head>
<body><header>
	
	<div id="avatar">
		<a href="https://pko89403.github.io/">
		  <img src="/img/Avatar.png" alt="유기농은 너무 비싸서 그런데 농약 친 건 어딨나요?">
		</a>
	</div>
	
	<div id="titletext"><h2 id="title"><a href="https://pko89403.github.io/">유기농은 너무 비싸서 그런데 농약 친 건 어딨나요?</a></h2></div>
	<div id="title-description"><p id="subtitle"><a href=""></a></p><div id="social">
			<nav>
				<ul>
					<li><a href="https://github.com/pko89403"><i title="Github" class="icons fab fa-github"></i></a></li><li><a href="mailto:pko89403@gmail.com"><i title="Email" class="icons fas fa-envelope"></i></a></li></ul>
			</nav>
		</div>
	</div>
	
	<div id="mainmenu">
		<nav>
			<ul>
				
				<li><a href="/">Home</a></li>
				
				<li><a href="/post">All Posts</a></li>
				
				<li><a href="/about">About</a></li>
				
				<li><a href="/tags">Tags</a></li>
				
				<li><a href="/categories">Categories</a></li>
				
			</ul>
		</nav>
	</div>
	
</header>
<main><div class="post">
	
	<div class="author">
	
	</div>
	<div class="post-header">
	
		<div class="meta">
			
			<div class="date">
				<span class="day">13</span>
				<span class="rest">Nov 2021</span>
			</div>
			
		</div>
		
		<div class="matter">
			<h1 class="title">밴딧 추천 오픈소스 OBP의 Off-Policy-Evaluation 에 대한 설명 : Off-Policy Evaluation</h1>
		</div>
	</div>
	<div class="markdown">
		<h1 id="밴딧-추천-오픈소스-obp의-off-policy-evaluation-에-대한-설명--off-policy-evaluation">&ldquo;밴딧 추천 오픈소스 OBP의 Off-Policy-Evaluation 에 대한 설명 : Off-Policy Evaluation&rdquo;</h1>
<p>Bandit 추천의 기반인 Policy에 대해 알아봤고 오프라인으로 로깅된 밴딧 피드백을 이용해 Policy의 값을 추정하는 추정기들과 그 추정기를 검증하는 Offline-Policy-Evaluation에 대해 실습을 진행한다. 아래의 공식 예제 코드를 약간 변형해서 진행하게 된다.</p>
<p><a href="https://github.com/st-tech/zr-obp/tree/master/examples/online" target="_blank">실습 진행 예제 코드</a></p>
<p>전체 코드의 실습은 OBP에서 지원하는 아래의 항목들을 사용했다.</p>
<ul>
<li>Dataset : SyntheticDataset (제공하는 랜덤 데이터셋 )</li>
<li>Policy : LinUCB</li>
<li>Evaluation : RM, IPW</li>
</ul>
<p>SyntheticBanditDataset을 온라인 밴딧 알고리즘의 OPE를 검증에 사용한다.  <br>
잘 알려진 off-policy 정책 추정기의 추정 성능을 검증하는데 synthetic data로 계산 가능한 검증 정책의 그라운드 트루스 정책 값을 사용한다.</p>
<p>Replay Method(RM)의 추정 성능을 검증한다.</p>
<h2 id="replay-methodrm">Replay Method(RM)</h2>
<p>로깅 밴딧 피드백 데이터의 서브셋을 사용하는데   <br>
이 로깅 데이터에서 선택된 액션들이 evaluation_policy와 같은 behavior_policy를 사용해서 선택된다.
RM은 behavior_policy와 evaluation_policy가 같은 action을 선택한 라운드 만을 사용한다.      <br>
behavior_policy가 유니폼 랜덤하게 액션을 선택하면 추정에 사용되는 데이터가 unbiased 하다고 설명할 수 있다.<br>
참고. 보통 evaluation_policy가 학습 알고리즘이면 잘 동작한다고 한다.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#080;font-weight:bold">from</span> <span style="color:#0e84b5;font-weight:bold">sklearn.linear_model</span> <span style="color:#080;font-weight:bold">import</span> LogisticRegression
<span style="color:#080;font-weight:bold">from</span> <span style="color:#0e84b5;font-weight:bold">obp.ope</span> <span style="color:#080;font-weight:bold">import</span> (
    OffPolicyEvaluation,
    ReplayMethod,
    InverseProbabilityWeighting <span style="color:#080;font-weight:bold">as</span> IPW,
    DirectMethod,
    RegressionModel
)
<span style="color:#080;font-weight:bold">from</span> <span style="color:#0e84b5;font-weight:bold">obp.policy</span> <span style="color:#080;font-weight:bold">import</span> LinUCB
<span style="color:#080;font-weight:bold">from</span> <span style="color:#0e84b5;font-weight:bold">obp.dataset</span> <span style="color:#080;font-weight:bold">import</span> SyntheticBanditDataset
<span style="color:#080;font-weight:bold">from</span> <span style="color:#0e84b5;font-weight:bold">obp.dataset</span> <span style="color:#080;font-weight:bold">import</span> logistic_reward_function
<span style="color:#080;font-weight:bold">from</span> <span style="color:#0e84b5;font-weight:bold">obp.simulator</span> <span style="color:#080;font-weight:bold">import</span> (
    run_bandit_simulation,
    calc_ground_truth_policy_value
)

</code></pre></div><div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#888"># ope_estimators = [ReplayMethod(), IPW(), DirectMethod()]</span>
ope_estimators <span style="color:#333">=</span> [ReplayMethod(), IPW()]
</code></pre></div><div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">n_runs <span style="color:#333">=</span> <span style="color:#00d;font-weight:bold">1</span>
n_rounds <span style="color:#333">=</span> <span style="color:#00d;font-weight:bold">10000</span>
n_actions <span style="color:#333">=</span> <span style="color:#00d;font-weight:bold">10</span>
dim_context <span style="color:#333">=</span> <span style="color:#00d;font-weight:bold">5</span>
n_sim <span style="color:#333">=</span> <span style="color:#00d;font-weight:bold">1</span>
evaluation_policy_name <span style="color:#333">=</span> <span style="background-color:#fff0f0">&#39;lin_ucb&#39;</span>
n_jobs <span style="color:#333">=</span> <span style="color:#00d;font-weight:bold">1</span>
random_seed <span style="color:#333">=</span> <span style="color:#00d;font-weight:bold">12345</span>

</code></pre></div><div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">evaluation_policy <span style="color:#333">=</span> LinUCB(dim<span style="color:#333">=</span>dim_context, n_actions<span style="color:#333">=</span>n_actions, random_state<span style="color:#333">=</span>random_seed)
</code></pre></div><p>uniformly random state 를 사용하는 synthetic dataset generator. <br>
오프라인 정책 검증을 위한 데이터셋을 생성한다.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">dataset <span style="color:#333">=</span> SyntheticBanditDataset(
    n_actions <span style="color:#333">=</span> n_actions,
    dim_context <span style="color:#333">=</span> dim_context, 
    reward_type<span style="color:#333">=</span><span style="background-color:#fff0f0">&#39;binary&#39;</span>,
    reward_function <span style="color:#333">=</span> logistic_reward_function, <span style="color:#888"># 랜덤하게 생성한 파라미터들로 sigmoid(logit) # reward_type도 있긴하다.</span>
    reward_std <span style="color:#333">=</span> <span style="color:#60e;font-weight:bold">1.0</span>,
    behavior_policy_function <span style="color:#333">=</span> None, <span style="color:#888"># Uniformly random </span>
    tau <span style="color:#333">=</span> <span style="color:#60e;font-weight:bold">1.0</span>,
    random_state <span style="color:#333">=</span> random_seed, 
    dataset_name<span style="color:#333">=</span><span style="background-color:#fff0f0">&#39;synthetic_bandit_dataset&#39;</span>
)
</code></pre></div><ol>
<li>behavior_policy 를 사용해서 action을 샘플링한다.</li>
<li>context를 이용해서 expected_reward를 reward_function을 사용해서 계산한다</li>
<li>scipy.truncnorm(잘린 정규 분포 : 확률 분포에 범위를 제한) 을 사용해서 expected_reward를 수정한다.</li>
<li>expected_reward를 사용해서 reward를 계산한다.</li>
</ol>
<p>behavior_policy에 None을 넣으면 Uniformly Random Sampling으로 action을 선택한다.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#080;font-weight:bold">import</span> <span style="color:#0e84b5;font-weight:bold">numpy</span> <span style="color:#080;font-weight:bold">as</span> <span style="color:#0e84b5;font-weight:bold">np</span> 
<span style="color:#080;font-weight:bold">import</span> <span style="color:#0e84b5;font-weight:bold">random</span> 

context <span style="color:#333">=</span> np<span style="color:#333">.</span>random<span style="color:#333">.</span>normal(size<span style="color:#333">=</span>(n_rounds, dim_context))
<span style="color:#080;font-weight:bold">print</span>(context<span style="color:#333">.</span>shape)
behavior_policy <span style="color:#333">=</span> np<span style="color:#333">.</span>ones(n_actions) <span style="color:#333">/</span> n_actions 
<span style="color:#080;font-weight:bold">print</span>(behavior_policy<span style="color:#333">.</span>shape)
<span style="color:#080;font-weight:bold">print</span>(behavior_policy)
behavior_policy <span style="color:#333">=</span> np<span style="color:#333">.</span>tile(behavior_policy, (n_rounds, <span style="color:#00d;font-weight:bold">1</span>))
<span style="color:#080;font-weight:bold">print</span>(behavior_policy<span style="color:#333">.</span>shape)
</code></pre></div><pre><code>(10000, 5)
(10,)
[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]
(10000, 10)
</code></pre>
<p>새로운 synthetic logged bandit feedback의 샘플링한다</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">bandit_feedback <span style="color:#333">=</span> dataset<span style="color:#333">.</span>obtain_batch_bandit_feedback(n_rounds<span style="color:#333">=</span>n_rounds)
</code></pre></div><div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">bandit_feedback<span style="color:#333">.</span>keys()
</code></pre></div><pre><code>dict_keys(['n_rounds', 'n_actions', 'context', 'action_context', 'action', 'position', 'reward', 'expected_reward', 'pscore'])
</code></pre>
<p>expected_reward와 reward의 차이</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">expected_reward_ = self.calc_expected_reward(context)
return self.sample_reward_given_expected_reward(expected_reward_, action)
</code></pre></div><p>Reward Type이 binomial 인지 continous 인지에 따라 random sampling 하는게 달라진다.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#888"># ML 모델로 평균 보상 함수를 추정한다. </span>
regression_model <span style="color:#333">=</span> RegressionModel(
    n_actions <span style="color:#333">=</span> dataset<span style="color:#333">.</span>n_actions,
    len_list <span style="color:#333">=</span> dataset<span style="color:#333">.</span>len_list, 
    action_context <span style="color:#333">=</span> dataset<span style="color:#333">.</span>action_context, 
    base_model <span style="color:#333">=</span> LogisticRegression(max_iter<span style="color:#333">=</span><span style="color:#00d;font-weight:bold">10000</span>, C<span style="color:#333">=</span><span style="color:#00d;font-weight:bold">100</span>, random_state<span style="color:#333">=</span><span style="color:#00d;font-weight:bold">12345</span>)
)

estimated_rewards_by_reg_model <span style="color:#333">=</span> regression_model<span style="color:#333">.</span>fit_predict(
    context <span style="color:#333">=</span> bandit_feedback[<span style="background-color:#fff0f0">&#39;context&#39;</span>],
    action <span style="color:#333">=</span> bandit_feedback[<span style="background-color:#fff0f0">&#39;action&#39;</span>], 
    reward <span style="color:#333">=</span> bandit_feedback[<span style="background-color:#fff0f0">&#39;reward&#39;</span>], 
    position <span style="color:#333">=</span> bandit_feedback[<span style="background-color:#fff0f0">&#39;position&#39;</span>], 
    pscore <span style="color:#333">=</span> bandit_feedback[<span style="background-color:#fff0f0">&#39;pscore&#39;</span>], 
    n_folds <span style="color:#333">=</span> <span style="color:#00d;font-weight:bold">3</span>, 
    random_state <span style="color:#333">=</span> random_seed
)

estimated_rewards_by_reg_model<span style="color:#333">.</span>shape
</code></pre></div><pre><code>(10000, 10, 1)
</code></pre>
<p>검증 정책(LinUCB)을 시뮬레이트 한다</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">action_dist <span style="color:#333">=</span> run_bandit_simulation(bandit_feedback<span style="color:#333">=</span>bandit_feedback, policy<span style="color:#333">=</span>evaluation_policy)
</code></pre></div><pre><code>100%|██████████| 10000/10000 [00:00&lt;00:00, 12022.68it/s]
</code></pre>
<p>검증 정책의 그라운드 트루스 정책 값을 추정한다 <br>
보상 분포는 p(r|x,a)를 사용한 몬테 카를로 결과를 사용한다</p>
<p>주어진 온라인 밴딧 알고리즘의 그라운드 트루스 정책 값을 몬테 카를로 시뮬레이션을 사용해서 계산한다
n_sim 만큼 전체 과정을 반복한다.</p>
<ul>
<li>정책을 초기화 한다. ( deepcopy )</li>
<li>bandit_feedback에 기록되어 있는 각 round 마다
<ul>
<li>검증 정책을 사용해 각 라운드의 액션을 선택한다.</li>
<li>검증 정책이 선택한 액션과 bandit_feedback[&lsquo;expected_reward&rsquo;]로 누적 보상을 계산한다.</li>
<li>검증 정책이 선택한 액션과 reward_sampler의 결과로 나온 reward를 사용해서 검증 정책을 업데이트한다.</li>
</ul>
</li>
<li>누적 보상 / ( n_sim * bandit_feedback[&lsquo;n_rounds&rsquo;] ) 를 리턴한다.</li>
</ul>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">ground_truth_policy_value <span style="color:#333">=</span> calc_ground_truth_policy_value(
    bandit_feedback <span style="color:#333">=</span> bandit_feedback,
    reward_sampler <span style="color:#333">=</span> dataset<span style="color:#333">.</span>sample_reward, <span style="color:#888"># p(r|x,a)</span>
    policy <span style="color:#333">=</span> evaluation_policy, 
    n_sim <span style="color:#333">=</span> n_sim <span style="color:#888"># 시뮬레이션 진행 횟수 </span>
)
</code></pre></div><pre><code>100%|██████████| 1/1 [00:03&lt;00:00,  3.91s/it]
</code></pre>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">ground_truth_policy_value
</code></pre></div><pre><code>0.7066556496362245
</code></pre>
<p>relative estimation error(relative-ee)를 사용해서 추정기의 성능을 검증한다</p>
<h1 id="offpolicyevaluation">OffPolicyEvaluation</h1>
<p>추정기 들로 OPE를 수행하기 위한 클래스</p>
<h2 id="파라미터">파라미터</h2>
<ul>
<li>bandit_feedback : OPE를 수행에 사용하는 밴딧 피드백 로그</li>
<li>ope_estimators : 검증 정책의 정책 값을 검증하는데 사용되는 OPE 추정기( obp.ope.BaseOffPolicyEstimator 인터페이스를 따라야함 ) 리스트</li>
</ul>
<h2 id="함수">함수</h2>
<p>기본적으로 추정기마다 그에 필요한 input의 형태를 dict 형태로 생성한 뒤(1번 함수) 필요한 추정기로 공급해준다. <br>
추정기 마다 구현된 estimate_policy_value와 estimated_intervals 함수를 통해 정책 값을 계산한다. <br>
이후에 3-8 번 함수들은 2,3번 함수의 결과값을 시각화 하거나 정리한 하는 함수이다.</p>
<p>action_dist와 estimated_rewards_by_reg_model 파라미터는 추정기마다 사용하는게 다른데  <br>
따라서 정책 값을 추정하는 방식의 차이이므로 기존에 추정기들이 어떤 방식으로 추정을 진행하는지에 대한 이해가 필요하다.  <br>
DM 이나 DR 추정기 같은 모델에 의존하는 추정기가 아니면, estimated_rewards_by_reg_model은 사용하지는 않는다.</p>
<ol>
<li>_create_estimator_inputs( action_dist, estimated_rewards_by_reg_model )</li>
<li>estimate_policy_value( action_dist, estimated_rewards_by_reg_model )</li>
<li>estimated_intervals( action_dist, estimated_rewards_by_reg_model, alpha, n_bootstrap_samples, random_state )</li>
<li>summarize_off_policy_estimates( action_dist, estimated_rewards_by_reg_model, alpha, n_bootstrap_samples, random_state )</li>
<li>visualize_off_policy_estimates ( action_dist, estimated_rewards_by_reg_model, alpha, is_relative, n_bootstrap_samples, random_state, fig_dir, fig_name )</li>
<li>evaluate_performance_of_estimators( ground_truth_policy_value, action_dist, estimated_rewards_by_reg_model, metric )</li>
<li>summarize_estimators_comparision( ground_truth_policy_value, action_dist, estimated_rewards_by_reg_model, metric )</li>
<li>visualize_off_policy_estimates_of_multiple_polices(policy_name_list, action_dist_list, estimated_rewards_by_reg_model, alpha, is_relative, n_bootstrap_samples, random_state, fig_dir, fig_name )</li>
</ol>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">ope <span style="color:#333">=</span> OffPolicyEvaluation(
    bandit_feedback<span style="color:#333">=</span>bandit_feedback, 
    ope_estimators <span style="color:#333">=</span> ope_estimators, 
)
</code></pre></div><h2 id="_create_estimator_inputs-action_dist-estimted_rewards_by_reg_model-">_create_estimator_inputs( action_dist, estimted_rewards_by_reg_model )</h2>
<p>ope_estimators에 정의한 추정기 마다 입력으로 사용할 action_dist 와 estimated_rewards_by_reg_model을 나눈다.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">estimator_input <span style="color:#333">=</span> ope<span style="color:#333">.</span>_create_estimator_inputs(action_dist<span style="color:#333">=</span>action_dist)
estimator_input
</code></pre></div><pre><code>{'rm': {'reward': array([1, 1, 1, ..., 0, 1, 0]),
  'action': array([6, 4, 2, ..., 9, 4, 7]),
  'position': array([0, 0, 0, ..., 0, 0, 0]),
  'pscore': array([0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1]),
  'action_dist': array([[[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [1.],
          [0.]]]),
  'estimated_rewards_by_reg_model': None},
 'ipw': {'reward': array([1, 1, 1, ..., 0, 1, 0]),
  'action': array([6, 4, 2, ..., 9, 4, 7]),
  'position': array([0, 0, 0, ..., 0, 0, 0]),
  'pscore': array([0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1]),
  'action_dist': array([[[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [1.],
          [0.]]]),
  'estimated_rewards_by_reg_model': None}}
</code></pre>
<h2 id="estimated_policy_values--action_dist-estimated_rewards_by_reg_model-">estimated_policy_values ( action_dist, estimated_rewards_by_reg_model )</h2>
<p>검증 정책의 정책 값을 추정한다. 추정기 들마다 구현한 estimate_policy_value 함수를 수행한 결과를 리턴한다.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">estimated_policy_value <span style="color:#333">=</span> ope<span style="color:#333">.</span>estimate_policy_values(action_dist<span style="color:#333">=</span>action_dist)
estimated_policy_value
</code></pre></div><pre><code>{'rm': 0.6407480314960629, 'ipw': 0.651}
</code></pre>
<h2 id="estimated_intervals-action_dist-estimated_rewards_by_reg_model-alpha-n_bootstrap_samples-random_state-">estimated_intervals( action_dist, estimated_rewards_by_reg_model, alpha, n_bootstrap_samples, random_state )</h2>
<p>정책 값의 신뢰 구간을 파라미터가 없는 부츠스트랩으로 추정한다. 추정기 들마다 구현한 estimated_interval 함수를 수행한 결과를 리턴한다.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">estimated_interval <span style="color:#333">=</span> ope<span style="color:#333">.</span>estimate_intervals(action_dist<span style="color:#333">=</span>action_dist)
estimated_interval

</code></pre></div><pre><code>{'rm': {'mean': 0.6431003937007873,
  '95.0% CI (lower)': 0.5939222440944882,
  '95.0% CI (upper)': 0.6924458661417323},
 'ipw': {'mean': 0.65035,
  '95.0% CI (lower)': 0.599475,
  '95.0% CI (upper)': 0.693525}}
</code></pre>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">estimated_policy_value, estimated_interval <span style="color:#333">=</span> ope<span style="color:#333">.</span>summarize_off_policy_estimates(action_dist<span style="color:#333">=</span>action_dist)
<span style="color:#080;font-weight:bold">print</span>(estimated_policy_value) <span style="color:#888"># relative_estimated_policy_value는 behavior policy의 정책 값을 나눈 값이다. </span>
<span style="color:#080;font-weight:bold">print</span>(estimated_interval)
</code></pre></div><pre><code>     estimated_policy_value  relative_estimated_policy_value
rm                 0.640748                         1.062248
ipw                0.651000                         1.079244
        mean  95.0% CI (lower)  95.0% CI (upper)
rm   0.64310          0.589518          0.712328
ipw  0.64436          0.593425          0.692200
</code></pre>
<h2 id="visualize_off_policy_estimates-action_dist-estimated_rewards_by_reg_model-alpha-is_relative-n_bootstrap_samples-random_state-fig_dir-fig_name-">visualize_off_policy_estimates( action_dist, estimated_rewards_by_reg_model, alpha, is_relative, n_bootstrap_samples, random_state, fig_dir, fig_name )</h2>
<ol>
<li>estimated_round_rewards 를 각 추정기 마다 구현된 _estimate_round_rewards를 사용해서 계산한다</li>
<li>seaborn.barplot 을 사용해서 round_rewards를 그린다</li>
</ol>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">visualize_policy_estimates <span style="color:#333">=</span> ope<span style="color:#333">.</span>visualize_off_policy_estimates(action_dist<span style="color:#333">=</span>action_dist)
</code></pre></div><figure>
    <img src="/images/obp/evaluate_33_0.png"/> <figcaption>
            <h4>visualize_off_policy_estimates(is_relative=False)</h4>
        </figcaption>
</figure>

<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">visualize_policy_estimates <span style="color:#333">=</span> ope<span style="color:#333">.</span>visualize_off_policy_estimates(action_dist<span style="color:#333">=</span>action_dist, is_relative<span style="color:#333">=</span>True)
</code></pre></div><figure>
    <img src="/images/obp/evaluate_34_0.png"/> <figcaption>
            <h4>visualize_off_policy_estimates(is_relative=True)</h4>
        </figcaption>
</figure>

<h2 id="evaluate_performance_of_estimators-ground_truth_policy_value-action_dist-estimated_rewards_by_reg_model-metric">evaluate_performance_of_estimators( ground_truth_policy_value, action_dist, estimated_rewards_by_reg_model, metric)</h2>
<p>OPE 추정기 들의 추정 성능을 검증한다. 추정기의 estimate_policy_value 함수로 정책 값을 추정한다</p>
<ul>
<li>Relative Estimation Error[ relative_ee ] := abs( (estimated - ground_truth) / ground_truth )</li>
<li>Squared Error [ se ] := ( estimated - ground_truth ) ** 2</li>
</ul>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">relative_ee_i <span style="color:#333">=</span> ope<span style="color:#333">.</span>evaluate_performance_of_estimators(
    ground_truth_policy_value<span style="color:#333">=</span>ground_truth_policy_value, 
    action_dist<span style="color:#333">=</span>action_dist, 
)
se_i <span style="color:#333">=</span> ope<span style="color:#333">.</span>evaluate_performance_of_estimators(
    ground_truth_policy_value<span style="color:#333">=</span>ground_truth_policy_value,
    action_dist<span style="color:#333">=</span>action_dist, 
    metric<span style="color:#333">=</span><span style="background-color:#fff0f0">&#39;se&#39;</span>
)
</code></pre></div><div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#080;font-weight:bold">print</span>(f<span style="background-color:#fff0f0">&#39;{relative_ee_i}&#39;</span>)


<span style="color:#080;font-weight:bold">print</span>(f<span style="background-color:#fff0f0">&#39;{se_i}&#39;</span>)
</code></pre></div><pre><code>{'rm': 0.0932669514127424, 'ipw': 0.07875922263534603}
{'rm': 0.004343814128909366, 'ipw': 0.0030975513364301754}
</code></pre>
<h2 id="summarize_estimators_comparision-ground_truth_policy_value-action_dist-estimated_rewards_by_reg_model-metric-">summarize_estimators_comparision( ground_truth_policy_value, action_dist, estimated_rewards_by_reg_model, metric )</h2>
<p>OPE 추정기들의 성능 비교를 요약한다</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">relative_ee_j <span style="color:#333">=</span> ope<span style="color:#333">.</span>summarize_estimators_comparison(ground_truth_policy_value<span style="color:#333">=</span>ground_truth_policy_value,
                                                    action_dist<span style="color:#333">=</span>action_dist)

relative_ee_j
</code></pre></div><!-- raw HTML omitted -->
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p><!-- raw HTML omitted --></p>
<!-- raw HTML omitted -->
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">ope<span style="color:#333">.</span>visualize_off_policy_estimates_of_multiple_policies(policy_name_list<span style="color:#333">=</span>[<span style="background-color:#fff0f0">&#39;Replay&#39;</span>, <span style="background-color:#fff0f0">&#39;IPW&#39;</span>],
                                                    action_dist_list<span style="color:#333">=</span>[action_dist, action_dist])
</code></pre></div><figure>
    <img src="/images/obp/evaluate_40_0.png"/> <figcaption>
            <h4>visualize_off_policy_estimates_of_multiple_policies</h4>
        </figcaption>
</figure>


	</div>
	
	
	
	
	
	
	
	<div class="tags">
		<div class="taxosfloating_left">
			<p>Categories</p>
		</div>
		<div class="termsfloating_right">
			<p>
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			<a href="/categories/recommendation/"> recommendation </a>
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			</p>
		</div>
		<div class="clearit"></div>
		
		
		
		
	
		
	
	
	
	<div class="tags">
		<div class="taxosfloating_left">
			<p>Tags</p>
		</div>
		<div class="termsfloating_right">
			<p>
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			<a href="/tags/bandit/"> bandit </a>
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			<a href="/tags/off-policy-evaluation/"> off-policy-evaluation </a>
			
			
			
			
			
			
			
			<a href="/tags/open-bandit-pipeline/"> open-bandit-pipeline </a>
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			<a href="/tags/recsys/"> recsys </a>
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			<a href="/tags/yahoo/"> yahoo </a>
			
			
			
			</p>
		</div>
		<div class="clearit"></div>
		
		
		
		
		
	</div>
<div id="disqus_thread"></div>
<script type="text/javascript">
	(function() {
	    
	    
	    if (window.location.hostname == "localhost")
	        return;
	    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
	    var disqus_shortname = 'Slave';
	    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	})();
</script>
<noscript>Please enable JavaScript to load the comments.</noscript>


</div>

  </main>
<footer>
	 © Copyright notice | <a href="https://github.com/dataCobra/hugo-vitae">Vitae</a> theme for <a href="https://gohugo.io">Hugo</a> 
	
	
	
</footer>


</body>
</html>
