<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>밴딧 추천 오픈소스 OBP의 Off-Policy-Evaluation 에 대한 설명 : Off-Policy Evaluation | 유기농은 너무 비싸서 그런데 농약 친 건 어딨나요?</title>
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="generator" content="Hugo 0.101.0" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="keywords" content="Recsys, Bandit, Yahoo, Open Bandit Pipeline, Off Policy Evaluation">
<meta name="description" content="“밴딧 추천 오픈소스 OBP의 Off-Policy-Evaluation 에 대한 설명 : Off-Policy Evaluation” Bandit 추천의 기반인 Policy에 대해 알아봤고 오프라인으로 로깅된 밴딧 피드백을 이용해 Policy의 값을 추정하는 추정기들과 그 추정기를  …">
<link rel="icon" type="image/png" href="/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/favicon-16x16.png" sizes="16x16">
<link rel="stylesheet" type="text/css" media="screen" href="/css/normalize.css" />
<link rel="stylesheet" type="text/css" media="screen" href="/css/main.css" />
<link rel="stylesheet" type="text/css" media="screen" href="/css/all.css" /><link rel="stylesheet" href="/css/katex.css" crossorigin="anonymous">
<script defer src="/js/katex.js"  integrity="sha384-HELAAZU8xvHgfT/8z4Mhmu+E2z3oBrMEuywaMh/CEd5uTZIDSct7TEaX+S43+dOi" crossorigin="anonymous"></script>
<script defer src="/js/auto-render.js" integrity="sha384-0QrOZ8WIARJUoT5pH7Iuzi7iq0mXUXLF9yj5ZvYxTkMRv1zcaPQiOQ9ksHYbKw7M" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
<script>
document.addEventListener("DOMContentLoaded", function() { renderMathInElement(document.body, { delimiters: [ {left: "$$", right: "$$", display: true}, {left: "$", right: "$", display: false} ] }); });
</script>




<meta property="og:title" content="밴딧 추천 오픈소스 OBP의 Off-Policy-Evaluation 에 대한 설명 : Off-Policy Evaluation" />
<meta property="og:description" content="&ldquo;밴딧 추천 오픈소스 OBP의 Off-Policy-Evaluation 에 대한 설명 : Off-Policy Evaluation&rdquo; Bandit 추천의 기반인 Policy에 대해 알아봤고 오프라인으로 로깅된 밴딧 피드백을 이용해 Policy의 값을 추정하는 추정기들과 그 추정기를 검증하는 Offline-Policy-Evaluation에 대해 실습을 진행한다. 아래의 공식 예제 코드를 약간 변형해서 진행하게 된다.
실습 진행 예제 코드
전체 코드의 실습은 OBP에서 지원하는 아래의 항목들을 사용했다.
Dataset : SyntheticDataset (제공하는 랜덤 데이터셋 ) Policy : LinUCB Evaluation : RM, IPW SyntheticBanditDataset을 온라인 밴딧 알고리즘의 OPE를 검증에 사용한다." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://pko89403.github.io/post/ope_offpolicy/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2021-11-13T16:05:47+09:00" />
<meta property="article:modified_time" content="2021-11-13T16:05:47+09:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="밴딧 추천 오픈소스 OBP의 Off-Policy-Evaluation 에 대한 설명 : Off-Policy Evaluation"/>
<meta name="twitter:description" content="&ldquo;밴딧 추천 오픈소스 OBP의 Off-Policy-Evaluation 에 대한 설명 : Off-Policy Evaluation&rdquo; Bandit 추천의 기반인 Policy에 대해 알아봤고 오프라인으로 로깅된 밴딧 피드백을 이용해 Policy의 값을 추정하는 추정기들과 그 추정기를 검증하는 Offline-Policy-Evaluation에 대해 실습을 진행한다. 아래의 공식 예제 코드를 약간 변형해서 진행하게 된다.
실습 진행 예제 코드
전체 코드의 실습은 OBP에서 지원하는 아래의 항목들을 사용했다.
Dataset : SyntheticDataset (제공하는 랜덤 데이터셋 ) Policy : LinUCB Evaluation : RM, IPW SyntheticBanditDataset을 온라인 밴딧 알고리즘의 OPE를 검증에 사용한다."/>

<meta itemprop="name" content="밴딧 추천 오픈소스 OBP의 Off-Policy-Evaluation 에 대한 설명 : Off-Policy Evaluation">
<meta itemprop="description" content="&ldquo;밴딧 추천 오픈소스 OBP의 Off-Policy-Evaluation 에 대한 설명 : Off-Policy Evaluation&rdquo; Bandit 추천의 기반인 Policy에 대해 알아봤고 오프라인으로 로깅된 밴딧 피드백을 이용해 Policy의 값을 추정하는 추정기들과 그 추정기를 검증하는 Offline-Policy-Evaluation에 대해 실습을 진행한다. 아래의 공식 예제 코드를 약간 변형해서 진행하게 된다.
실습 진행 예제 코드
전체 코드의 실습은 OBP에서 지원하는 아래의 항목들을 사용했다.
Dataset : SyntheticDataset (제공하는 랜덤 데이터셋 ) Policy : LinUCB Evaluation : RM, IPW SyntheticBanditDataset을 온라인 밴딧 알고리즘의 OPE를 검증에 사용한다."><meta itemprop="datePublished" content="2021-11-13T16:05:47+09:00" />
<meta itemprop="dateModified" content="2021-11-13T16:05:47+09:00" />
<meta itemprop="wordCount" content="1083">
<meta itemprop="keywords" content="Recsys,Bandit,Yahoo,Open Bandit Pipeline,Off Policy Evaluation," />
</head>
<body>
<header>
<div id="avatar">
<a href="https://pko89403.github.io/"><img src="/img/Avatar.png" alt="유기농은 너무 비싸서 그런데 농약 친 건 어딨나요?"></a>
</div>
<div id="titletext">
<h2 id="title"><a href="https://pko89403.github.io/">유기농은 너무 비싸서 그런데 농약 친 건 어딨나요?</a></h2>
</div>
<div id="title-description">
<p id="subtitle"><a href=""></a></p>
<div id="social">
<nav><ul>
<li><a href="https://github.com/pko89403"><i title="Github" class="icons fab fa-github"></i></a></li>
<li><a href="mailto:pko89403@gmail.com"><i title="Email" class="icons fas fa-envelope"></i></a></li>
<li><a><i title="Switch Dark Mode" class="dark-mode icons fas fa-moon"></i></a></li>
</ul></nav>
</div>
</div>
<div id="mainmenu">
<nav>
<ul>
<li><a href="/">Home</a></li>
<li><a href="/post">All Posts</a></li>
<li><a href="/about">About</a></li>
<li><a href="/tags">Tags</a></li>
<li><a href="/categories">Categories</a></li>
</ul>
</nav>
</div>
</header>
<main>
<div class="post">
<article>
<div class="post-header">
<div class="meta">
<div class="date">
<span class="day">13</span>
<span class="rest">Nov 2021</span>
</div>
</div>
<div class="matter">
<h1 class="title">밴딧 추천 오픈소스 OBP의 Off-Policy-Evaluation 에 대한 설명 : Off-Policy Evaluation</h1>
<p class="post-meta">
<span class="post-meta">





</span>

</p>
</div>
</div>
<div class="markdown">
<h1 id="밴딧-추천-오픈소스-obp의-off-policy-evaluation-에-대한-설명--off-policy-evaluation">&ldquo;밴딧 추천 오픈소스 OBP의 Off-Policy-Evaluation 에 대한 설명 : Off-Policy Evaluation&rdquo;</h1>
<p>Bandit 추천의 기반인 Policy에 대해 알아봤고 오프라인으로 로깅된 밴딧 피드백을 이용해 Policy의 값을 추정하는 추정기들과 그 추정기를 검증하는 Offline-Policy-Evaluation에 대해 실습을 진행한다. 아래의 공식 예제 코드를 약간 변형해서 진행하게 된다.</p>
<p><a href="https://github.com/st-tech/zr-obp/tree/master/examples/online" target="_blank">실습 진행 예제 코드</a></p>
<p>전체 코드의 실습은 OBP에서 지원하는 아래의 항목들을 사용했다.</p>
<ul>
<li>Dataset : SyntheticDataset (제공하는 랜덤 데이터셋 )</li>
<li>Policy : LinUCB</li>
<li>Evaluation : RM, IPW</li>
</ul>
<p>SyntheticBanditDataset을 온라인 밴딧 알고리즘의 OPE를 검증에 사용한다.  <br>
잘 알려진 off-policy 정책 추정기의 추정 성능을 검증하는데 synthetic data로 계산 가능한 검증 정책의 그라운드 트루스 정책 값을 사용한다.</p>
<p>Replay Method(RM)의 추정 성능을 검증한다.</p>
<h2 id="replay-methodrm">Replay Method(RM)</h2>
<p>로깅 밴딧 피드백 데이터의 서브셋을 사용하는데   <br>
이 로깅 데이터에서 선택된 액션들이 evaluation_policy와 같은 behavior_policy를 사용해서 선택된다.
RM은 behavior_policy와 evaluation_policy가 같은 action을 선택한 라운드 만을 사용한다.      <br>
behavior_policy가 유니폼 랜덤하게 액션을 선택하면 추정에 사용되는 데이터가 unbiased 하다고 설명할 수 있다.<br>
참고. 보통 evaluation_policy가 학습 알고리즘이면 잘 동작한다고 한다.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#080;font-weight:bold">from</span> <span style="color:#0e84b5;font-weight:bold">sklearn.linear_model</span> <span style="color:#080;font-weight:bold">import</span> LogisticRegression
</span></span><span style="display:flex;"><span><span style="color:#080;font-weight:bold">from</span> <span style="color:#0e84b5;font-weight:bold">obp.ope</span> <span style="color:#080;font-weight:bold">import</span> (
</span></span><span style="display:flex;"><span>    OffPolicyEvaluation,
</span></span><span style="display:flex;"><span>    ReplayMethod,
</span></span><span style="display:flex;"><span>    InverseProbabilityWeighting <span style="color:#080;font-weight:bold">as</span> IPW,
</span></span><span style="display:flex;"><span>    DirectMethod,
</span></span><span style="display:flex;"><span>    RegressionModel
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span><span style="color:#080;font-weight:bold">from</span> <span style="color:#0e84b5;font-weight:bold">obp.policy</span> <span style="color:#080;font-weight:bold">import</span> LinUCB
</span></span><span style="display:flex;"><span><span style="color:#080;font-weight:bold">from</span> <span style="color:#0e84b5;font-weight:bold">obp.dataset</span> <span style="color:#080;font-weight:bold">import</span> SyntheticBanditDataset
</span></span><span style="display:flex;"><span><span style="color:#080;font-weight:bold">from</span> <span style="color:#0e84b5;font-weight:bold">obp.dataset</span> <span style="color:#080;font-weight:bold">import</span> logistic_reward_function
</span></span><span style="display:flex;"><span><span style="color:#080;font-weight:bold">from</span> <span style="color:#0e84b5;font-weight:bold">obp.simulator</span> <span style="color:#080;font-weight:bold">import</span> (
</span></span><span style="display:flex;"><span>    run_bandit_simulation,
</span></span><span style="display:flex;"><span>    calc_ground_truth_policy_value
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#888"># ope_estimators = [ReplayMethod(), IPW(), DirectMethod()]</span>
</span></span><span style="display:flex;"><span>ope_estimators <span style="color:#333">=</span> [ReplayMethod(), IPW()]
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>n_runs <span style="color:#333">=</span> <span style="color:#00d;font-weight:bold">1</span>
</span></span><span style="display:flex;"><span>n_rounds <span style="color:#333">=</span> <span style="color:#00d;font-weight:bold">10000</span>
</span></span><span style="display:flex;"><span>n_actions <span style="color:#333">=</span> <span style="color:#00d;font-weight:bold">10</span>
</span></span><span style="display:flex;"><span>dim_context <span style="color:#333">=</span> <span style="color:#00d;font-weight:bold">5</span>
</span></span><span style="display:flex;"><span>n_sim <span style="color:#333">=</span> <span style="color:#00d;font-weight:bold">1</span>
</span></span><span style="display:flex;"><span>evaluation_policy_name <span style="color:#333">=</span> <span style="background-color:#fff0f0">&#39;lin_ucb&#39;</span>
</span></span><span style="display:flex;"><span>n_jobs <span style="color:#333">=</span> <span style="color:#00d;font-weight:bold">1</span>
</span></span><span style="display:flex;"><span>random_seed <span style="color:#333">=</span> <span style="color:#00d;font-weight:bold">12345</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>evaluation_policy <span style="color:#333">=</span> LinUCB(dim<span style="color:#333">=</span>dim_context, n_actions<span style="color:#333">=</span>n_actions, random_state<span style="color:#333">=</span>random_seed)
</span></span></code></pre></div><p>uniformly random state 를 사용하는 synthetic dataset generator. <br>
오프라인 정책 검증을 위한 데이터셋을 생성한다.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>dataset <span style="color:#333">=</span> SyntheticBanditDataset(
</span></span><span style="display:flex;"><span>    n_actions <span style="color:#333">=</span> n_actions,
</span></span><span style="display:flex;"><span>    dim_context <span style="color:#333">=</span> dim_context, 
</span></span><span style="display:flex;"><span>    reward_type<span style="color:#333">=</span><span style="background-color:#fff0f0">&#39;binary&#39;</span>,
</span></span><span style="display:flex;"><span>    reward_function <span style="color:#333">=</span> logistic_reward_function, <span style="color:#888"># 랜덤하게 생성한 파라미터들로 sigmoid(logit) # reward_type도 있긴하다.</span>
</span></span><span style="display:flex;"><span>    reward_std <span style="color:#333">=</span> <span style="color:#60e;font-weight:bold">1.0</span>,
</span></span><span style="display:flex;"><span>    behavior_policy_function <span style="color:#333">=</span> <span style="color:#080;font-weight:bold">None</span>, <span style="color:#888"># Uniformly random </span>
</span></span><span style="display:flex;"><span>    tau <span style="color:#333">=</span> <span style="color:#60e;font-weight:bold">1.0</span>,
</span></span><span style="display:flex;"><span>    random_state <span style="color:#333">=</span> random_seed, 
</span></span><span style="display:flex;"><span>    dataset_name<span style="color:#333">=</span><span style="background-color:#fff0f0">&#39;synthetic_bandit_dataset&#39;</span>
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><ol>
<li>behavior_policy 를 사용해서 action을 샘플링한다.</li>
<li>context를 이용해서 expected_reward를 reward_function을 사용해서 계산한다</li>
<li>scipy.truncnorm(잘린 정규 분포 : 확률 분포에 범위를 제한) 을 사용해서 expected_reward를 수정한다.</li>
<li>expected_reward를 사용해서 reward를 계산한다.</li>
</ol>
<p>behavior_policy에 None을 넣으면 Uniformly Random Sampling으로 action을 선택한다.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#080;font-weight:bold">import</span> <span style="color:#0e84b5;font-weight:bold">numpy</span> <span style="color:#080;font-weight:bold">as</span> <span style="color:#0e84b5;font-weight:bold">np</span> 
</span></span><span style="display:flex;"><span><span style="color:#080;font-weight:bold">import</span> <span style="color:#0e84b5;font-weight:bold">random</span> 
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>context <span style="color:#333">=</span> np<span style="color:#333">.</span>random<span style="color:#333">.</span>normal(size<span style="color:#333">=</span>(n_rounds, dim_context))
</span></span><span style="display:flex;"><span><span style="color:#007020">print</span>(context<span style="color:#333">.</span>shape)
</span></span><span style="display:flex;"><span>behavior_policy <span style="color:#333">=</span> np<span style="color:#333">.</span>ones(n_actions) <span style="color:#333">/</span> n_actions 
</span></span><span style="display:flex;"><span><span style="color:#007020">print</span>(behavior_policy<span style="color:#333">.</span>shape)
</span></span><span style="display:flex;"><span><span style="color:#007020">print</span>(behavior_policy)
</span></span><span style="display:flex;"><span>behavior_policy <span style="color:#333">=</span> np<span style="color:#333">.</span>tile(behavior_policy, (n_rounds, <span style="color:#00d;font-weight:bold">1</span>))
</span></span><span style="display:flex;"><span><span style="color:#007020">print</span>(behavior_policy<span style="color:#333">.</span>shape)
</span></span></code></pre></div><pre><code>(10000, 5)
(10,)
[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]
(10000, 10)
</code></pre>
<p>새로운 synthetic logged bandit feedback의 샘플링한다</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>bandit_feedback <span style="color:#333">=</span> dataset<span style="color:#333">.</span>obtain_batch_bandit_feedback(n_rounds<span style="color:#333">=</span>n_rounds)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>bandit_feedback<span style="color:#333">.</span>keys()
</span></span></code></pre></div><pre><code>dict_keys(['n_rounds', 'n_actions', 'context', 'action_context', 'action', 'position', 'reward', 'expected_reward', 'pscore'])
</code></pre>
<p>expected_reward와 reward의 차이</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>expected_reward_ = self.calc_expected_reward(context)
</span></span><span style="display:flex;"><span>return self.sample_reward_given_expected_reward(expected_reward_, action)
</span></span></code></pre></div><p>Reward Type이 binomial 인지 continous 인지에 따라 random sampling 하는게 달라진다.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#888"># ML 모델로 평균 보상 함수를 추정한다. </span>
</span></span><span style="display:flex;"><span>regression_model <span style="color:#333">=</span> RegressionModel(
</span></span><span style="display:flex;"><span>    n_actions <span style="color:#333">=</span> dataset<span style="color:#333">.</span>n_actions,
</span></span><span style="display:flex;"><span>    len_list <span style="color:#333">=</span> dataset<span style="color:#333">.</span>len_list, 
</span></span><span style="display:flex;"><span>    action_context <span style="color:#333">=</span> dataset<span style="color:#333">.</span>action_context, 
</span></span><span style="display:flex;"><span>    base_model <span style="color:#333">=</span> LogisticRegression(max_iter<span style="color:#333">=</span><span style="color:#00d;font-weight:bold">10000</span>, C<span style="color:#333">=</span><span style="color:#00d;font-weight:bold">100</span>, random_state<span style="color:#333">=</span><span style="color:#00d;font-weight:bold">12345</span>)
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>estimated_rewards_by_reg_model <span style="color:#333">=</span> regression_model<span style="color:#333">.</span>fit_predict(
</span></span><span style="display:flex;"><span>    context <span style="color:#333">=</span> bandit_feedback[<span style="background-color:#fff0f0">&#39;context&#39;</span>],
</span></span><span style="display:flex;"><span>    action <span style="color:#333">=</span> bandit_feedback[<span style="background-color:#fff0f0">&#39;action&#39;</span>], 
</span></span><span style="display:flex;"><span>    reward <span style="color:#333">=</span> bandit_feedback[<span style="background-color:#fff0f0">&#39;reward&#39;</span>], 
</span></span><span style="display:flex;"><span>    position <span style="color:#333">=</span> bandit_feedback[<span style="background-color:#fff0f0">&#39;position&#39;</span>], 
</span></span><span style="display:flex;"><span>    pscore <span style="color:#333">=</span> bandit_feedback[<span style="background-color:#fff0f0">&#39;pscore&#39;</span>], 
</span></span><span style="display:flex;"><span>    n_folds <span style="color:#333">=</span> <span style="color:#00d;font-weight:bold">3</span>, 
</span></span><span style="display:flex;"><span>    random_state <span style="color:#333">=</span> random_seed
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>estimated_rewards_by_reg_model<span style="color:#333">.</span>shape
</span></span></code></pre></div><pre><code>(10000, 10, 1)
</code></pre>
<p>검증 정책(LinUCB)을 시뮬레이트 한다</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>action_dist <span style="color:#333">=</span> run_bandit_simulation(bandit_feedback<span style="color:#333">=</span>bandit_feedback, policy<span style="color:#333">=</span>evaluation_policy)
</span></span></code></pre></div><pre><code>100%|██████████| 10000/10000 [00:00&lt;00:00, 12022.68it/s]
</code></pre>
<p>검증 정책의 그라운드 트루스 정책 값을 추정한다 <br>
보상 분포는 p(r|x,a)를 사용한 몬테 카를로 결과를 사용한다</p>
<p>주어진 온라인 밴딧 알고리즘의 그라운드 트루스 정책 값을 몬테 카를로 시뮬레이션을 사용해서 계산한다
n_sim 만큼 전체 과정을 반복한다.</p>
<ul>
<li>정책을 초기화 한다. ( deepcopy )</li>
<li>bandit_feedback에 기록되어 있는 각 round 마다
<ul>
<li>검증 정책을 사용해 각 라운드의 액션을 선택한다.</li>
<li>검증 정책이 선택한 액션과 bandit_feedback[&rsquo;expected_reward&rsquo;]로 누적 보상을 계산한다.</li>
<li>검증 정책이 선택한 액션과 reward_sampler의 결과로 나온 reward를 사용해서 검증 정책을 업데이트한다.</li>
</ul>
</li>
<li>누적 보상 / ( n_sim * bandit_feedback[&rsquo;n_rounds&rsquo;] ) 를 리턴한다.</li>
</ul>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>ground_truth_policy_value <span style="color:#333">=</span> calc_ground_truth_policy_value(
</span></span><span style="display:flex;"><span>    bandit_feedback <span style="color:#333">=</span> bandit_feedback,
</span></span><span style="display:flex;"><span>    reward_sampler <span style="color:#333">=</span> dataset<span style="color:#333">.</span>sample_reward, <span style="color:#888"># p(r|x,a)</span>
</span></span><span style="display:flex;"><span>    policy <span style="color:#333">=</span> evaluation_policy, 
</span></span><span style="display:flex;"><span>    n_sim <span style="color:#333">=</span> n_sim <span style="color:#888"># 시뮬레이션 진행 횟수 </span>
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><pre><code>100%|██████████| 1/1 [00:03&lt;00:00,  3.91s/it]
</code></pre>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>ground_truth_policy_value
</span></span></code></pre></div><pre><code>0.7066556496362245
</code></pre>
<p>relative estimation error(relative-ee)를 사용해서 추정기의 성능을 검증한다</p>
<h1 id="offpolicyevaluation">OffPolicyEvaluation</h1>
<p>추정기 들로 OPE를 수행하기 위한 클래스</p>
<h2 id="파라미터">파라미터</h2>
<ul>
<li>bandit_feedback : OPE를 수행에 사용하는 밴딧 피드백 로그</li>
<li>ope_estimators : 검증 정책의 정책 값을 검증하는데 사용되는 OPE 추정기( obp.ope.BaseOffPolicyEstimator 인터페이스를 따라야함 ) 리스트</li>
</ul>
<h2 id="함수">함수</h2>
<p>기본적으로 추정기마다 그에 필요한 input의 형태를 dict 형태로 생성한 뒤(1번 함수) 필요한 추정기로 공급해준다. <br>
추정기 마다 구현된 estimate_policy_value와 estimated_intervals 함수를 통해 정책 값을 계산한다. <br>
이후에 3-8 번 함수들은 2,3번 함수의 결과값을 시각화 하거나 정리한 하는 함수이다.</p>
<p>action_dist와 estimated_rewards_by_reg_model 파라미터는 추정기마다 사용하는게 다른데  <br>
따라서 정책 값을 추정하는 방식의 차이이므로 기존에 추정기들이 어떤 방식으로 추정을 진행하는지에 대한 이해가 필요하다.  <br>
DM 이나 DR 추정기 같은 모델에 의존하는 추정기가 아니면, estimated_rewards_by_reg_model은 사용하지는 않는다.</p>
<ol>
<li>_create_estimator_inputs( action_dist, estimated_rewards_by_reg_model )</li>
<li>estimate_policy_value( action_dist, estimated_rewards_by_reg_model )</li>
<li>estimated_intervals( action_dist, estimated_rewards_by_reg_model, alpha, n_bootstrap_samples, random_state )</li>
<li>summarize_off_policy_estimates( action_dist, estimated_rewards_by_reg_model, alpha, n_bootstrap_samples, random_state )</li>
<li>visualize_off_policy_estimates ( action_dist, estimated_rewards_by_reg_model, alpha, is_relative, n_bootstrap_samples, random_state, fig_dir, fig_name )</li>
<li>evaluate_performance_of_estimators( ground_truth_policy_value, action_dist, estimated_rewards_by_reg_model, metric )</li>
<li>summarize_estimators_comparision( ground_truth_policy_value, action_dist, estimated_rewards_by_reg_model, metric )</li>
<li>visualize_off_policy_estimates_of_multiple_polices(policy_name_list, action_dist_list, estimated_rewards_by_reg_model, alpha, is_relative, n_bootstrap_samples, random_state, fig_dir, fig_name )</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>ope <span style="color:#333">=</span> OffPolicyEvaluation(
</span></span><span style="display:flex;"><span>    bandit_feedback<span style="color:#333">=</span>bandit_feedback, 
</span></span><span style="display:flex;"><span>    ope_estimators <span style="color:#333">=</span> ope_estimators, 
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><h2 id="_create_estimator_inputs-action_dist-estimted_rewards_by_reg_model-">_create_estimator_inputs( action_dist, estimted_rewards_by_reg_model )</h2>
<p>ope_estimators에 정의한 추정기 마다 입력으로 사용할 action_dist 와 estimated_rewards_by_reg_model을 나눈다.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>estimator_input <span style="color:#333">=</span> ope<span style="color:#333">.</span>_create_estimator_inputs(action_dist<span style="color:#333">=</span>action_dist)
</span></span><span style="display:flex;"><span>estimator_input
</span></span></code></pre></div><pre><code>{'rm': {'reward': array([1, 1, 1, ..., 0, 1, 0]),
  'action': array([6, 4, 2, ..., 9, 4, 7]),
  'position': array([0, 0, 0, ..., 0, 0, 0]),
  'pscore': array([0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1]),
  'action_dist': array([[[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [1.],
          [0.]]]),
  'estimated_rewards_by_reg_model': None},
 'ipw': {'reward': array([1, 1, 1, ..., 0, 1, 0]),
  'action': array([6, 4, 2, ..., 9, 4, 7]),
  'position': array([0, 0, 0, ..., 0, 0, 0]),
  'pscore': array([0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1]),
  'action_dist': array([[[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [1.],
          [0.]]]),
  'estimated_rewards_by_reg_model': None}}
</code></pre>
<h2 id="estimated_policy_values--action_dist-estimated_rewards_by_reg_model-">estimated_policy_values ( action_dist, estimated_rewards_by_reg_model )</h2>
<p>검증 정책의 정책 값을 추정한다. 추정기 들마다 구현한 estimate_policy_value 함수를 수행한 결과를 리턴한다.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>estimated_policy_value <span style="color:#333">=</span> ope<span style="color:#333">.</span>estimate_policy_values(action_dist<span style="color:#333">=</span>action_dist)
</span></span><span style="display:flex;"><span>estimated_policy_value
</span></span></code></pre></div><pre><code>{'rm': 0.6407480314960629, 'ipw': 0.651}
</code></pre>
<h2 id="estimated_intervals-action_dist-estimated_rewards_by_reg_model-alpha-n_bootstrap_samples-random_state-">estimated_intervals( action_dist, estimated_rewards_by_reg_model, alpha, n_bootstrap_samples, random_state )</h2>
<p>정책 값의 신뢰 구간을 파라미터가 없는 부츠스트랩으로 추정한다. 추정기 들마다 구현한 estimated_interval 함수를 수행한 결과를 리턴한다.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>estimated_interval <span style="color:#333">=</span> ope<span style="color:#333">.</span>estimate_intervals(action_dist<span style="color:#333">=</span>action_dist)
</span></span><span style="display:flex;"><span>estimated_interval
</span></span></code></pre></div><pre><code>{'rm': {'mean': 0.6431003937007873,
  '95.0% CI (lower)': 0.5939222440944882,
  '95.0% CI (upper)': 0.6924458661417323},
 'ipw': {'mean': 0.65035,
  '95.0% CI (lower)': 0.599475,
  '95.0% CI (upper)': 0.693525}}
</code></pre>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>estimated_policy_value, estimated_interval <span style="color:#333">=</span> ope<span style="color:#333">.</span>summarize_off_policy_estimates(action_dist<span style="color:#333">=</span>action_dist)
</span></span><span style="display:flex;"><span><span style="color:#007020">print</span>(estimated_policy_value) <span style="color:#888"># relative_estimated_policy_value는 behavior policy의 정책 값을 나눈 값이다. </span>
</span></span><span style="display:flex;"><span><span style="color:#007020">print</span>(estimated_interval)
</span></span></code></pre></div><pre><code>     estimated_policy_value  relative_estimated_policy_value
rm                 0.640748                         1.062248
ipw                0.651000                         1.079244
        mean  95.0% CI (lower)  95.0% CI (upper)
rm   0.64310          0.589518          0.712328
ipw  0.64436          0.593425          0.692200
</code></pre>
<h2 id="visualize_off_policy_estimates-action_dist-estimated_rewards_by_reg_model-alpha-is_relative-n_bootstrap_samples-random_state-fig_dir-fig_name-">visualize_off_policy_estimates( action_dist, estimated_rewards_by_reg_model, alpha, is_relative, n_bootstrap_samples, random_state, fig_dir, fig_name )</h2>
<ol>
<li>estimated_round_rewards 를 각 추정기 마다 구현된 _estimate_round_rewards를 사용해서 계산한다</li>
<li>seaborn.barplot 을 사용해서 round_rewards를 그린다</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>visualize_policy_estimates <span style="color:#333">=</span> ope<span style="color:#333">.</span>visualize_off_policy_estimates(action_dist<span style="color:#333">=</span>action_dist)
</span></span></code></pre></div><figure><img src="/images/obp/evaluate_33_0.png"/><figcaption>
            <h4>visualize_off_policy_estimates(is_relative=False)</h4>
        </figcaption>
</figure>

<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>visualize_policy_estimates <span style="color:#333">=</span> ope<span style="color:#333">.</span>visualize_off_policy_estimates(action_dist<span style="color:#333">=</span>action_dist, is_relative<span style="color:#333">=</span><span style="color:#080;font-weight:bold">True</span>)
</span></span></code></pre></div><figure><img src="/images/obp/evaluate_34_0.png"/><figcaption>
            <h4>visualize_off_policy_estimates(is_relative=True)</h4>
        </figcaption>
</figure>

<h2 id="evaluate_performance_of_estimators-ground_truth_policy_value-action_dist-estimated_rewards_by_reg_model-metric">evaluate_performance_of_estimators( ground_truth_policy_value, action_dist, estimated_rewards_by_reg_model, metric)</h2>
<p>OPE 추정기 들의 추정 성능을 검증한다. 추정기의 estimate_policy_value 함수로 정책 값을 추정한다</p>
<ul>
<li>Relative Estimation Error[ relative_ee ] := abs( (estimated - ground_truth) / ground_truth )</li>
<li>Squared Error [ se ] := ( estimated - ground_truth ) ** 2</li>
</ul>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>relative_ee_i <span style="color:#333">=</span> ope<span style="color:#333">.</span>evaluate_performance_of_estimators(
</span></span><span style="display:flex;"><span>    ground_truth_policy_value<span style="color:#333">=</span>ground_truth_policy_value, 
</span></span><span style="display:flex;"><span>    action_dist<span style="color:#333">=</span>action_dist, 
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>se_i <span style="color:#333">=</span> ope<span style="color:#333">.</span>evaluate_performance_of_estimators(
</span></span><span style="display:flex;"><span>    ground_truth_policy_value<span style="color:#333">=</span>ground_truth_policy_value,
</span></span><span style="display:flex;"><span>    action_dist<span style="color:#333">=</span>action_dist, 
</span></span><span style="display:flex;"><span>    metric<span style="color:#333">=</span><span style="background-color:#fff0f0">&#39;se&#39;</span>
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#007020">print</span>(<span style="background-color:#fff0f0">f</span><span style="background-color:#fff0f0">&#39;</span><span style="background-color:#eee">{</span>relative_ee_i<span style="background-color:#eee">}</span><span style="background-color:#fff0f0">&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#007020">print</span>(<span style="background-color:#fff0f0">f</span><span style="background-color:#fff0f0">&#39;</span><span style="background-color:#eee">{</span>se_i<span style="background-color:#eee">}</span><span style="background-color:#fff0f0">&#39;</span>)
</span></span></code></pre></div><pre><code>{'rm': 0.0932669514127424, 'ipw': 0.07875922263534603}
{'rm': 0.004343814128909366, 'ipw': 0.0030975513364301754}
</code></pre>
<h2 id="summarize_estimators_comparision-ground_truth_policy_value-action_dist-estimated_rewards_by_reg_model-metric-">summarize_estimators_comparision( ground_truth_policy_value, action_dist, estimated_rewards_by_reg_model, metric )</h2>
<p>OPE 추정기들의 성능 비교를 요약한다</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>relative_ee_j <span style="color:#333">=</span> ope<span style="color:#333">.</span>summarize_estimators_comparison(ground_truth_policy_value<span style="color:#333">=</span>ground_truth_policy_value,
</span></span><span style="display:flex;"><span>                                                    action_dist<span style="color:#333">=</span>action_dist)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>relative_ee_j
</span></span></code></pre></div><!-- raw HTML omitted -->
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p><!-- raw HTML omitted --></p>
<!-- raw HTML omitted -->
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>ope<span style="color:#333">.</span>visualize_off_policy_estimates_of_multiple_policies(policy_name_list<span style="color:#333">=</span>[<span style="background-color:#fff0f0">&#39;Replay&#39;</span>, <span style="background-color:#fff0f0">&#39;IPW&#39;</span>],
</span></span><span style="display:flex;"><span>                                                    action_dist_list<span style="color:#333">=</span>[action_dist, action_dist])
</span></span></code></pre></div><figure><img src="/images/obp/evaluate_40_0.png"/><figcaption>
            <h4>visualize_off_policy_estimates_of_multiple_policies</h4>
        </figcaption>
</figure>


</div>
<div class="tags">
<div class="taxosfloating_left">
<p>Categories</p>
</div>
<div class="termsfloating_right">
<p>
<a href="/categories/recommendation/">recommendation</a>
</p>
</div>
<div class="clearit"></div>
<div class="tags">
<div class="taxosfloating_left">
<p>Tags</p>
</div>
<div class="termsfloating_right">
<p>
<a href="/tags/bandit/">bandit</a>
<a href="/tags/off-policy-evaluation/">off-policy-evaluation</a>
<a href="/tags/open-bandit-pipeline/">open-bandit-pipeline</a>
<a href="/tags/recsys/">recsys</a>
<a href="/tags/yahoo/">yahoo</a>
</p>
</div>
<div class="clearit"></div>
</div>
<div id="disqus_thread"></div>
<script type="text/javascript">
(function() {


if (window.location.hostname == "localhost")
  return;
var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
var disqus_shortname = 'Slave';
dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();
</script>
<noscript>Please enable JavaScript to load the comments.</noscript>


</article>
</div>
</main>
<footer>
© Copyright notice | <a href="https://github.com/dataCobra/hugo-vitae" target="_blank">Vitae</a> theme for <a href="https://gohugo.io" target="_blank">Hugo</a>
</footer><script src="/js/dark-mode.js"></script>

</body>
</html>
